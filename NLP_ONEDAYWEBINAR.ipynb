{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-ONEDAYWEBINAR.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6xvEgMARho7"
      },
      "outputs": [],
      "source": [
        "name=\"my name is sanjeev\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "name.split()"
      ],
      "metadata": {
        "id": "9vPZHmwOR-Dg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b53ed692-3e19-4cab-ec86-607ac078a1cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['my', 'name', 'is', 'sanjeev']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"my name is sanjeev . i am studying in uwindsor.\""
      ],
      "metadata": {
        "id": "So85Wc5hSZFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text.split('.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXOOgUVXS5wc",
        "outputId": "bd464e5e-fb1a-4b6f-d2e7-f2a31c0f5676"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['my name is sanjeev ', ' i am studying in uwindsor', '']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "using regex"
      ],
      "metadata": {
        "id": "t6ZpfTzCTdec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "s75ugFpYTZqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"who are you ? my name is sanjeev . i am studying in uwindsor .\""
      ],
      "metadata": {
        "id": "nY9-xB2gTc5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=re.compile('[.?]').split(text)"
      ],
      "metadata": {
        "id": "j6WGCwSJS82m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OjeluxJT1Cx",
        "outputId": "11915ab9-8682-4cd2-b954-ad807c6e7d8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['who are you ', ' my name is sanjeev ', ' i am studying in uwindsor ', '']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "using nltk"
      ],
      "metadata": {
        "id": "KX4GghQ4U6wu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "pjdnaXLEUGiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize "
      ],
      "metadata": {
        "id": "V8EprAkdU_VJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmPvlTDTVOti",
        "outputId": "c19a52b0-9962-4140-a458-3c904a5796c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"who are you ? my name is sanjeev . i am studying in uwindsor .\""
      ],
      "metadata": {
        "id": "SNncN-u-Viyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l1=word_tokenize(text)\n",
        "l1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXBTVDJNVUKF",
        "outputId": "6c3cb58e-1c44-44b2-f913-6b5ff2982491"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['who',\n",
              " 'are',\n",
              " 'you',\n",
              " '?',\n",
              " 'my',\n",
              " 'name',\n",
              " 'is',\n",
              " 'sanjeev',\n",
              " '.',\n",
              " 'i',\n",
              " 'am',\n",
              " 'studying',\n",
              " 'in',\n",
              " 'uwindsor',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "assignment- 3 tokenization function"
      ],
      "metadata": {
        "id": "NeLHtBHnWCx6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "id": "YBeXkGyzVbCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eloG42ZvWSlr",
        "outputId": "ad45560e-d51d-4898-dc29-51832a932b36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['who are you ?', 'my name is sanjeev .', 'i am studying in uwindsor .']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dPLD9AUYWWdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "stemming- lowers the inflection in words"
      ],
      "metadata": {
        "id": "seQVe-fhWfgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "aOp_VrC8WqVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "porter=PorterStemmer()"
      ],
      "metadata": {
        "id": "OuM-UkU_XIrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words=['wait','waiting','waited','waits']"
      ],
      "metadata": {
        "id": "J7PO6K0jXTRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "  print(word,\"-->\",porter.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMGYmi_EXeXl",
        "outputId": "f40e26ff-1ca3-44ba-db81-58afe7fa035c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wait --> wait\n",
            "waiting --> wait\n",
            "waited --> wait\n",
            "waits --> wait\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "snowballStemmer"
      ],
      "metadata": {
        "id": "BSQa2FzfZVOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer"
      ],
      "metadata": {
        "id": "FkygikJLX7v9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "porter=SnowballStemmer(language=\"english\")"
      ],
      "metadata": {
        "id": "KDGTwyf5ZuMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in l1:\n",
        "  print(word,\"-->\",porter.stem(word))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TFVmW4KZiNI",
        "outputId": "e87c92e5-50b4-458c-9f25-7a2baa84bd35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "who --> who\n",
            "are --> are\n",
            "you --> you\n",
            "? --> ?\n",
            "my --> my\n",
            "name --> name\n",
            "is --> is\n",
            "sanjeev --> sanjeev\n",
            ". --> .\n",
            "i --> i\n",
            "am --> am\n",
            "studying --> studi\n",
            "in --> in\n",
            "uwindsor --> uwindsor\n",
            ". --> .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "lemmatization-similar to stemming -"
      ],
      "metadata": {
        "id": "QexAm0Y1gWbe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob, Word"
      ],
      "metadata": {
        "id": "OB5Fw93bgRsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xh9hBcn7aBcv",
        "outputId": "ce5976aa-1c2a-406c-e087-cc5f0c7dfc44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "listofwords=['waiting','plays','files','borrowed']"
      ],
      "metadata": {
        "id": "C3elXSy3hGLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in listofwords:\n",
        "  w=Word(word)\n",
        "  print(word,\"-->\",w.lemmatize())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8vvrJ4uhUdc",
        "outputId": "8f724adc-79dd-4551-94e7-b50c2fe15cdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "waiting --> waiting\n",
            "plays --> play\n",
            "files --> file\n",
            "borrowed --> borrowed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "Go4_VHdXiGtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wn1=WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "_sR8Y5l4iUrx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in listofwords:\n",
        "  print(word + '-->' + wn1.lemmatize(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_2-Ui9CiZaa",
        "outputId": "03799d8b-3b24-4312-d820-c8b6b172f815"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "waiting-->waiting\n",
            "plays-->play\n",
            "files-->file\n",
            "borrowed-->borrowed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**vectorize-**\n",
        "bag of words \n",
        "#**Cosine similarity**\n",
        "\n",
        "\n",
        "*   term frequency-inverse term frequency\n",
        "*   (documents/sentence/query)->all are same\n",
        "\n",
        "\n",
        "\n",
        "##**TF score**->number of times term occurs in document/total no. of terms in document\n",
        "##**IDF score**->DF(t)= log[n/df(t)]  \n",
        "n-total number of documents,\n",
        "df(t)-frequency of term t in all documents.\n",
        "#**vector representation** -> tf*idf\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "In-a8q1gmHhV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**word embeddings**\n",
        "it tokenize words into numbers \n",
        "#word2vec\n",
        "it automatically vectorize the words in document"
      ],
      "metadata": {
        "id": "5PObu_xmtOY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import en_core_web_sm"
      ],
      "metadata": {
        "id": "RAuB3AkjmG5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp=en_core_web_sm.load()"
      ],
      "metadata": {
        "id": "OXeUaUDAirSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent1=nlp(\"how can i check my account balance\")\n",
        "print(sent1)\n",
        "sent2=nlp(\"I want to check my account balance\")\n",
        "print(sent2)\n",
        "print(sent2.similarity(sent1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nu1Ndpruuz97",
        "outputId": "ef093285-57b2-49c2-f6d3-7a8eafef1cd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "how can i check my account balance\n",
            "I want to check my account balance\n",
            "0.8028908779206821\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "stopwords are the words which does not have any big meaning or weightage in the sentence , so we can remove it"
      ],
      "metadata": {
        "id": "o2V8UqB-wsUN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiBVdFwovRD1",
        "outputId": "0c069631-6c85-4510-c0b2-a6fa30283595"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk \n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "_AXZhQf7wFS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qzw7MscBwW43",
        "outputId": "e0735568-45f9-4d14-d213-9e76a4c2b333"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkgPNwolwgtD",
        "outputId": "9f4335f6-dece-47f5-ce68-ad6c85ff25f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "from nltk.tokenize import word_tokenize\n",
        "word_tokens_1=word_tokenize(\"how can i check my account balance\")\n",
        "word_tokens_1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3LpoeN4w_39",
        "outputId": "6b0be723-605e-4bcf-d32f-12c2cbcc9942"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['how', 'can', 'i', 'check', 'my', 'account', 'balance']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "going to eleminate the words that are in the list of stop words"
      ],
      "metadata": {
        "id": "PurUwmVRywRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_sent_1=[w for w in word_tokens_1 if w.lower() not in stop_words]\n",
        "print(new_sent_1)\n",
        "new_sent_1=' '.join(str(i) for i in new_sent_1)\n",
        "print(new_sent_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePl8kUq_xoMF",
        "outputId": "fd016c50-2c49-42b7-b7b7-0dcd847f8dcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['check', 'account', 'balance']\n",
            "check account balance\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokens_2=word_tokenize(\"i want to check my account balance\")\n",
        "sent_2=[w for w in word_tokens_2 if w.lower() not in stop_words]\n",
        "print(sent_2)\n",
        "new_sent_2=' '.join(str(i) for i in sent_2)\n",
        "print(new_sent_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvTN9IOe1LBv",
        "outputId": "2a3aa5e0-2132-4c8e-ab19-c6bee92502a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['want', 'check', 'account', 'balance']\n",
            "want check account balance\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_sent_1=nlp(new_sent_1)\n",
        "new_sent_2=nlp(new_sent_2)\n",
        "print(new_sent_2.similarity(new_sent_1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dFBHA4CyCWg",
        "outputId": "2662d4e9-0509-4145-dc58-1555fddacf25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9003613092570767\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "g-Hn_fCa0wYQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}